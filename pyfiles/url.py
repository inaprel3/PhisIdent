# pyfiles/url.py
from datasets import load_dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report
import joblib
import os

print("üî∑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É URL...")
url_dataset = load_dataset("ealvaradob/phishing-dataset", "urls", trust_remote_code=True)
df_main = url_dataset['train'].to_pandas()

print("–ü–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É:")
print(df_main.head())

print("\nüî∑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –∑ csv/custom_urls.csv...")
custom_csv_path = os.path.join(os.path.dirname(__file__), '..', 'csv', 'custom_urls.csv')
df_custom = pd.read_csv(custom_csv_path)

print("–ü–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É:")
print(df_custom.head())

df = pd.concat([df_main, df_custom], ignore_index=True)
print(f"\n–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ –ø—ñ—Å–ª—è –æ–±‚Äô—î–¥–Ω–∞–Ω–Ω—è: {len(df)}")

X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=5000)
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

rf_model = RandomForestClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)

print("\n--- Random Forest ---")
rf_model.fit(X_train_vect, y_train)
print(classification_report(y_test, rf_model.predict(X_test_vect)))

print("\n--- Gradient Boosting ---")
gb_model.fit(X_train_vect, y_train)
print(classification_report(y_test, gb_model.predict(X_test_vect)))

joblib.dump(rf_model, 'pkl/url_rf_model.pkl')
print("‚úÖ Random Forest –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —è–∫ 'url_rf_model.pkl'")

joblib.dump(gb_model, 'pkl/url_gb_model.pkl')
print("‚úÖ Gradient Boosting –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —è–∫ 'url_gb_model.pkl'")

joblib.dump(vectorizer, 'pkl/url_vectorizer.pkl')
print("‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫ 'url_vectorizer.pkl'")
